{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# **Module 1 - Data Wrangling**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "In this module we will be introduced to Jupyter Notebooks and learn how to read in, view, and edit data using the pandas library in Python, but first, why use Python over Excell, Google Sheets, or other similar software?\n",
                "\n",
                "## When to use Python instead of Excel\n",
                "Excel is excellent for quickly looking through your data and building simple charts, formulas, and models. However, using an actual coding language gives you the ability to easily edit and save large data sets and to build more complex functions. This can be useful when:\n",
                "* Merging or extracting data from multiple files\n",
                "* Automating repetitive data processing \n",
                "* Fitting multiple models\n",
                "* Writing complex formulas\n",
                "* Cleaning dirty data \n",
                "\n",
                "## What is a Jupyter Notebook\n",
                "A Jupyter Notebook is an interactive environment for writing code. A Jupyter notebook is made up of cells with each cell containing either text or code. Cells are run one at a time allowing the user to run and test individual chunks of code. They are useful in data analysis where tasks such as reading in data or training models may take several minutes and you don't want to have to re run all of your code every time you make a change. \n",
                "\n",
                "Cody cells are run by either clicking on the \"play\" icon to the top left of the cell or by holding **shift** and pressing **enter** on the keyboard. When you run a cell, it will usually display the output of the last line of code below the cell. In this notebook, we change the setting so that the output of every line is displayed for demonstration purposes. \n",
                "\n",
                "In Jupyter notebooks the order in which you run the code cells, not the order in which you the code cells are listed, dictates how the code is evaluated. This can cause errors if you reuse variable names in multiple code cells and then run the cells out of order. \n",
                "\n",
                "(For example, if in cell 1 contains the code **A=\"dog\"**, and cell 2 we contains  **A=2** and **B=5**, and cell 3 contains **A+B**, we can run all three cells in order and obtain an answer of 7. However, if we run cell 2, then cell 1, and then cell 3 we will receive and error when the string \"dog\" and the integer 5 are added together.)\n",
                "\n",
                "## How to open Jupyter Notebooks\n",
                "There are three main ways we recommend running Jupyter Notebooks:\n",
                "* Using Google Collab to run notebook online - this requires the least set up and technical \"know how\". It requires a Google amount. Here is a quick video showing how.\n",
                "  https://www.youtube.com/watch?v=R3sKKvMCwTo&ab_channel=RobMulla\n",
                "* Running Jupyter Notebooks locally through VS Code - this is our favorite way but it does require a bit of installation to get it running. Visit this website for instructions.\n",
                "https://code.visualstudio.com/docs/datascience/jupyter-notebooks\n",
                "* Running Jupyter Notebooks using JupyterLab - this the official way to run Jupyter Notebooks, but requires the most manual set up and management. This video gives in depth information on what Jupyter Notebooks are and how to set up JupyterLab as well as going over Google Collab and Kaggle Notebooks. \n",
                "https://www.youtube.com/watch?v=5pf0_bpNbkw&ab_channel=RobMulla"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [],
            "source": [
                "#import pandas and numpy\n",
                "import pandas as pd \n",
                "import numpy as np\n",
                "\n",
                "#Base Python does not contain every function you could ever want to use. Instead certain functions are grouped into packages that you can import when you want to use them.\n",
                "#If you have never used pandas before, you will first need to install it.\n",
                "#Youtube, Google, or ChatGPT can all tell you how to install Python packages "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reading in Your Data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Pandas has a variety of \"read\" functions you can use to read in different types of files. The two you are most likely to use and that we will be covering here are CSV and Excel (.xlsx) files. CSV stand for Comma Separated Value. CSV store tabular data with columns separated by commas and rows separated by line. CSV files can be opened in Excel and saved as Excel Workbooks (.xlsx). Excel Workbooks contain tabular data like CSV files, but they can also store much more data including multiple sheets, formulas, charts, and formatting (colors or custom column widths). You can save Excel Workbooks as CSV files, but only the tabular data from the selected sheet will be saved. Graphs, formatting, and additional sheets will be lost. \n",
                "\n",
                "Here we read in an Excel file using pd.read_excel and a csv file using pd.read_csv. We can also use pd.read_csv to read in other delimitated file types such as .dat and .txt files. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_xlsx = pd.read_excel(\"hotel_bookings.xlsx\") #import Excel file\n",
                "df_csv = pd.read_csv(\"weatherHistory.csv\") #import csv file\n",
                "\n",
                "# Other delimitated file types\n",
                "df_dat = pd.read_csv(\"DataLogger.dat\", delimiter=\",\") #import dat file\n",
                "df_txt = pd.read_csv(\"TextExample.txt\", delimiter=\"\\t\") #import txt file\n",
                "\n",
                "# We will primarily be working with the weatherHistory.csv, so we resave that as df for easy access\n",
                "df = df_csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Pandas DataFrames\n",
                "Above we have read our data into a Pandas DataFrame. DataFrames are two dimensional tables with labeled rows and columns. They are built to be able to effcinalty store and manage data. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Exploring Your DataFrame\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Displaying DataFrames\n",
                "If you are new to pandas, opening up your csv or xlsx file in Excel will probably be the most comfortable way to browse over it. However, there are a couple  of options in pandas to do this as well. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "First off you can simply type the name of your DataFrame into a cell and run it. However, by default it will only display 10 rows and 20 columns of data with elipses to indicate the missing rows and columns. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#displays the csv DataFrame \n",
                "df"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Another way to get a sneak peak at your data is the .head(n) function. It prints the first n rows of your DataFrame. Ran without and argument it will default to printing 5 rows. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "But maybe you really just want to see what the column and row names are. In that case, you can use df.columns and df.index. If you are curious as to why these commands don't include () after them, read the syntax note in the next section. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.columns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "One last way to get a peak at our DataFrame is to use value_counts to see what is in each column. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[\"Summary\"].value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sizing Up Your Dataframe (and a note on syntax)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "There are several ways to look at the size of your DataFrame. You may have noticed that the output from \"df\" printed the number of rows and columns at the end of the output, but the .shape and other attributes allows you to access the number of rows and columns in a way you can directly use.\n",
                "\n",
                "**Syntax Note**\\\n",
                "Going through this section you will also notice what (initially) seems like wildly inconsistent syntax. Most of what we do with DataFrames will involve methods which utilize the **df.method()** syntax. We saw this above with df.head() which could take an optional attribute inside its parenthesis. Other methods we will be learning include df.mean() and df.sum(). This syntax indicates pandas methods specific to DataFrames. \n",
                "\n",
                "DataFrames also have attributes which are properties of the DataFrame which can be accessed with the **df.attribute** syntax. If you've learned about classes in Python, you may remember that accessing an attribute retrieves a piece of data inherent to the object. Below I will list all of a DataFrame's attributes (per ChatGPT). Some we will cover later. \n",
                "\n",
                "Finally we have functions which use the **function(df)** syntax. The len() function is probably the only function you wil ever use on a DataFrame (kind of annoying in our opinion). It uses this syntax because it is a built in python function instead of a method specific to pandas. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**All DataFrame Attributes (per ChatGPT)**\n",
                "<ul>\n",
                "<li>df.shape: Returns a tuple representing the dimensionality of the DataFrame (number of rows, number of columns).\n",
                "<li>df.size: Returns an integer representing the number of elements in the DataFrame (i.e., rows Ã— columns).\n",
                "<li>df.ndim: Returns the number of dimensions of the DataFrame, which is always 2 for a DataFrame (rows and columns).\n",
                "<li>df.columns: Returns an Index object containing the column labels of the DataFrame.\n",
                "<li>df.index: Returns an Index object containing the row labels (index) of the DataFrame.\n",
                "<li>df.dtypes: Returns a Series with the data type of each column in the DataFrame.\n",
                "<li>df.values: Returns a Numpy array containing the underlying data of the DataFrame.\n",
                "<li>df.T: Transposes the DataFrame, swapping rows and columns.\n",
                "<li>df.empty: Returns a boolean indicating whether the DataFrame is empty (i.e., contains no elements).\n",
                "<li>df.axes: Returns a list of the row axis (index) and column axis (columns) labels.\n",
                "<li>df.name: If the DataFrame was created from a named Series, this attribute stores the name of the Series.\n",
                "\n",
                "</ul>"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Accessing Values\n",
                "There are several ways to access specific values in a DataFrame. \n",
                "\n",
                "First, you can access individual columns or specific elements using brackets and column and row names in the same way you would for a nested list. While this notation may be the most familiar, it isn't as commonly used as the next two we will cover. \n",
                "\n",
                "Note that the column name is in parenthesis because it is a string, while our row name is not since it is a numbered index. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['Humidity'][4]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.columns"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## .at[] and .loc[] - Row and Column Names\n",
                "\n",
                ".at[] and .loc[] are the preferred ways of accessing values based on their labels (column names and row/index names or values).\n",
                "\n",
                "Unlike the bracket notation, they take in row information and then column information. \n",
                "\n",
                ".at[row_label, column_label] returns a single value\n",
                "\n",
                ".loc[row_label, column_label] will also return a single value, but using the full syntax .loc[\"start row\":\"end row\", \"start column\":\"end column\"] you can return ranges of values as well. Empty column or row information indicates the whole row or column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.at[4, 'Humidity']\n",
                "df.loc[4, 'Humidity']\n",
                "segment = df.loc[4:20, 'Temperature (C)':'Humidity']\n",
                "segment\n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## .iat[] and .iloc[] - Row and Column Indices\n",
                ".iat[] and .iloc[] function almost same as .at[] and .loc[] expect they use the index numbers and column indices (instead of names) and for .iloc[] the output is non-inclusive of the top end of the range. Note that the code below only returns 16 rows (rows 4-19), while the code above returned 17 rows (rows 4-20)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.iloc[4:20, 3:5]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Editing DataFrames\n",
                "## Replacing Values\n",
                "\n",
                "Using at[], loc[], iat[], and iloc[] you can create new DataFrames or replace existing values. There are also specialized methods that let you edit other parts of your DataFrame."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Creating a new DataFrame from a portion of df\")\n",
                "df_segment = df.iloc[2:5,2:5]\n",
                "print(df_segment)\n",
                "\n",
                "print(\"replacing a value using .iat (uses row and column numbers)\")\n",
                "df_segment.iat[0,0] = \"I changed\"\n",
                "print(df_segment)\n",
                "\n",
                "print(\"Replacing a section using .loc\")\n",
                "df_segment.loc[3, 'Precip Type':'Apparent Temperature (C)'] = [\"new row\", \"new row\", \"new row\"]\n",
                "print(df_segment)\n",
                "# Note the warning we get when this last section of code runs. We are inserting string data types into columns that consist of integers. \n",
                "# Pandas warns that this is deprecated and will cause an error in future versions. If this code doesn't run, try replacing with:\n",
                "# df_segment.loc[3, 'Precip Type':'Apparent Temperature (C)'] = [\"new row\", 0, 0]\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Creating Columns\n",
                "\n",
                "You can perform various mathematical operations on columns to either change the column or create a new column.\n",
                "\n",
                "Pandas does not edit DataFrames in place, meaning that running the code df[\"column name\"]*2 will not change df[\"column name\"], it will simply return the value of 2 times df[\"column name\"].\n",
                "To save this value, either assign it to df[\"column name\"] to change df[\"column name\"], or assign it to a new column. \n",
                "\n",
                "Below we use mathematical functions to make a new columns \"Temperature (F)\" and \"Apparent Temperature (F)\" from the \"Temperature (C)\" and \"Apparent Temperature (C)\" columns.\\\n",
                "This allows us to view the temperature in Fahrenheit instead Celsius conversion. \n",
                "\n",
                "Note that the new columns are found at the end of the dataset. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df[\"Temperature (F)\"] = df[\"Temperature (C)\"]*9/5 + 32\n",
                "df[\"Apparent Temperature (F)\"] = df[\"Apparent Temperature (C)\"]*9/5 + 32\n",
                "df.head()\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Dropping Columns and Rows\n",
                "\n",
                "We can drop columns or rows using the df.drop() function. We must specify both a list containing the rows and columns we wish to drop and an axis of either 0 for rows, or 1 for columns. \n",
                "\n",
                "Below we decide to make a new DataFrame containing only temperatures in Fahrenheit. We make sure to create a new DataFrame, df_Fahrenheit, instead of overwriting df because otherwise re-running the previous code cell will result in an error when df[\"Temperature (C)\"] and df[\"Apparent Temperature (C)\"] can not be found."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_Fahrenheit = df.drop([\"Temperature (C)\",\"Apparent Temperature (C)\"], axis = 1)\n",
                "df_Fahrenheit.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Filtering Based on Attributes\n",
                "\n",
                "We can also filter or even create new columns in a DataFrame by creating True and False statements.\n",
                "\n",
                "Below we create a DataFrame \"High_Wind\" that consists only of rows where df[\"Wind Speed (km/h)\"] > 15. Trying running just the code df[\"Wind Speed (km/h)\"] > 15 and see the logical vector of True and False values it creates. This list is used to create high wind. Rows with \"True\" are selected and rows with \"False\" are not. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "High_Wind = df[df[\"Wind Speed (km/h)\"] > 15] # selects rows where (df[\"arrival_date_month\"] == \"July\") is True\n",
                "High_Wind"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## np.select() to create new columns\n",
                "We've noticed that the descriptions in \"Summary\" contains either the word \"windy\", \"breezy\", or no mention of mind at all. We would like to create a new columns \"Wind Type\" that lists either \"windy\", \"breezy\", or \"still\" to reflect this. \n",
                "\n",
                "First, we use .value_counts() to see all the options for \"Summary\".\n",
                "Then we create variables windy_conditions and breezy_conditions to store the descriptions we are interested in.\n",
                "Last we use np.select() to make a new column \"Wind Type\". The first list we pass it contains a list of conditions we are interested in, the second list specifies what values the new column should have if these conditions are found, and \"default\" specifies the value for all rows not already specified. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_Fahrenheit.value_counts(\"Summary\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "windy_conditions = (df_Fahrenheit[\"Summary\"] == \"Windy and Partly Cloudy\") | (df_Fahrenheit[\"Summary\"] == \"Windy and Overcast\") | (df_Fahrenheit[\"Summary\"] == \"Windy and Mostly Cloudy\") | (df_Fahrenheit[\"Summary\"] == \"Windy\") | (df_Fahrenheit[\"Summary\"] == \"Windy and Dry\")\n",
                "breezy_conditions = (df[\"Summary\"] == \"Breezy and Overcast\") | (df_Fahrenheit[\"Summary\"] == \"Breezy and Mostly Cloudy\") | (df_Fahrenheit[\"Summary\"] == \"Breezy and Partly Cloudy\") | (df_Fahrenheit[\"Summary\"] == \"Breezy\") | (df_Fahrenheit[\"Summary\"] == \"Breezy and Foggy\") | (df_Fahrenheit[\"Summary\"] == \"Breezy and Dry\")\n",
                "df_Fahrenheit[\"Wind Type\"] = np.select([windy_conditions,breezy_conditions],[\"windy\",\"breezy\"],default = \"still\")\n",
                "df_Fahrenheit.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Sorting your Dataframe\n",
                "\n",
                "You can sort your DataFrame either by index using .sort_idex() or by column value using .sort_values(). Try running each bit of the code below by itself to see what happens. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#To sort by index \n",
                "df = df.sort_index() # sorts in descending order\n",
                "df.head()\n",
                "df = df.sort_index(ascending = True) # sorts in ascending order\n",
                "df.head()\n",
                "\n",
                "#To sort by a column \n",
                "df_sorted = df.sort_values(by='Wind Speed (km/h)')    # sorts in descending order\n",
                "df.head()\n",
                "df_sorted = df.sort_values(by='Wind Speed (km/h)', ascending=False)   # sorts in ascending order\n",
                "df_sorted.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We've noticed that our DataFrame is not currently in chronological order. We can see this by looking at rows 20-30 where it jumps from 2006-04-01 23:00:00.000 +0200 to 2006-04-10 00:00:00.000 +0200 instead of going to the next day 2006-04-02.\n",
                "\n",
                "We would like to sort our DataFrame chronologically, so first we use pd.to_datatime() to change the values in \"Formatted Date\" to Datetime objects, then we sort by this column. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.iloc[20:30]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_Fahrenheit[\"Formatted Date\"] = pd.to_datetime(df_Fahrenheit['Formatted Date'], utc = True)   #   utc = True accounts for varying timezones. \n",
                "df_Fahrenheit = df.sort_values(by = \"Formatted Date\")\n",
                "df_Fahrenheit = df_Fahrenheit.reset_index(drop=True)    # reset index to be in numerical order\n",
                "df_Fahrenheit.iloc[20:30]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Saving DataFrame as a new File \n",
                "\n",
                "In the next module, we will be interested in graphing various aspects of this dataset. However, there is currently SOOO much data. Here we will make a new DataFrame which only contains the measurement taken at noon of each day and save that to a new csv file using .to_csv()."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_noon = df_Fahrenheit[df_Fahrenheit.index%24 == 12] # selects the 12th, 32th, 58th.... indices which correspond to noon\n",
                "df_noon = df_noon.reset_index(drop=True)    # reset index again\n",
                "df_noon.to_csv(\"weatherCondensed.csv\", index=False)     # output csv file"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Reading in Multiple Data Files \n",
                "Some test equipment creates a new CSV every time a data point is taken. To be able to utilize this data, it must be combined into a single DataFrame. \n",
                "\n",
                "The following function assumes you are interested in extracting one column from each of your data files. It will take this column and insert it as a row in a new DataFrame. Depending on the format of your data, this structure may need to be changed. \n",
                "\n",
                "The function asks for the number of files to be read in [num_files], the folder they are located in [folder], any file prefix (for example \"test\" if you files were named test1, test2...) [prefix], the number of zeros the numbers in your files are padded with [zero], the name of the column you want to extract [column], a starting and ending row to extract from [start_row, end_row], and the name of your output file [ouput_file_name]."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def combine_files(num_files, folder, prefix, zeros, column, start_row, end_row, output_file_name):\n",
                "    #create data frame of NaN  values with specified size \n",
                "    df = pd.DataFrame(np.nan, index=range(num_files), columns= range(start_row,end_row+1))\n",
                "    #loops through each file and extracts the specified column \n",
                "    for i in range(num_files):\n",
                "        temp = pd.read_csv(folder+\"/\"+prefix+ str(i).zfill(zeros)+\".csv\")\n",
                "        df.iloc[i] = temp.loc[start_row:end_row, column].to_numpy().reshape(1, -1)\n",
                "    #writes the finished dataframe to a new file \n",
                "    df.to_csv(output_file_name, index=False)\n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "combine_files(11, \"material_data\", \"\",4, \" \\\"x\\\"\", 2, 502, \"combined_file\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Credits\n",
                "Thank you to Dr. Carly Fox from Utah State University for allowing us to reference her DATA 3300 course materials while creating this module. "
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
